# Sample script on how to parse SEEL logs generated by "SEEL_gateway_node.ino"

# This script assumes NODEs are run uninterrupted. If NODES are unplugged, paused, and then re-plugged,
# the script will state that NODE missed BCASTS during the duration of the pause. Quick power cycles are okay. 
# If node join entries are not captured in the logs (logging created on an established network), node IDs can be added in the "Hardcoded Section"

# Tested with Python 3.5.2

# Requires installation of sklearn: "pip3 install sklearn"

# To run: python3 <path_to_this_file>/SEEL_log_parser.py <path_to_data_file>/<data_file> (optional)<path to param file>/<param file>

# Expected input format (All <> is 1 byte):
# Broadcast time: "BT: <time>"
# Broadcast data: "BD: <INDEX BCAST 0> <INDEX BCAST 1> ....
# Node data: <INDEX DATA 0> <INDEX DATA 1> ....

# Make sure the "Indexes Section" in this script matches that in "SEEL_Defines.h" and "SEEL_sensor_node.ino"

# Use the deployment info file argument to override default parser parameters. See the "External Variables Section" for more info.

import os
import sys
import math
import importlib
import numpy as np
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
import networkx as nx
import statistics

class Parameters:
    ############################################################################
    # General Parameters
    PRINT_ALL_MSGS = False

    PLOT_DISPLAY = False
    
    # Per node plots
    PLOT_NODE_SPECIFIC_BCASTS = False
    PLOT_NODE_SPECIFIC_CONNECTIONS = False
    PLOT_NODE_SPECIFIC_MAPS = False
    PLOT_NODE_PARENT_CYCLE_RSSI = False
    
    PLOT_RSSI_ANALYSIS = False
    PLOT_LOCS_WEIGHT_SCALAR = 1000 # Smaller for thicker lines
    PLOT_LOCS_WEIGHT_SCALAR_SPECIFIC = 500 # Smaller for thicker lines

    PARAM_COUNT_WRAP_SAFETY = 15 # Send count will not have wrapped within this many counts, keep it lower to account for node restarts too

    ############################################################################
    # Hardcode Section
    HC_NJ_ORIGINAL_ID_IDX = 0
    HC_NJ_ASSIGNED_ID_IDX = 1
    HC_NJ_CYCLE_JOIN_IDX = 2
    HARDCODED_NODE_JOINS = [
        # Format -> [actual ID, assigned ID, cycle join]
    ]

    HARDCODED_NODE_LOCS = {
        # Format -> node ID: (loc_x, loc_y)
    }
    
    # Node TDMA slots
    HARDCODED_NODE_TDMA = {
        # Format -> node ID: TDMA slot
    }
    
    # Excludes nodes from correlation plots
    # Useful for any outliers that may skew regressions
    HARDCODED_PLOT_EXCLUDE = {
        # Format -> node_id
    }
    
    NETWORK_DRAW_OPTIONS = {
        "node_font_size": 10,
        "node_size": 250,
        "node_color": "white",
        "node_edge_color": "black",
        "node_width": 1,
        "edge_width": 1,
    }

    ############################################################################
    # Indexes Section
    INDEX_HEADER = 0
    # INDEX_BT 0 used for the text "BT:"
    INDEX_BT_TIME = 1
    # INDEX_BD 0 used for the text "BD:"
    INDEX_BD_FIRST = 1
    INDEX_BD_BCAST_COUNT = 2
    INDEX_BD_SYS_TIME_0 = 3
    INDEX_BD_SYS_TIME_1 = 4
    INDEX_BD_SYS_TIME_2 = 5
    INDEX_BD_SYS_TIME_3 = 6
    INDEX_BD_SNODE_AWAKE_TIME_0 = 7
    INDEX_BD_SNODE_AWAKE_TIME_1 = 8
    INDEX_BD_SNODE_AWAKE_TIME_2 = 9
    INDEX_BD_SNODE_AWAKE_TIME_3 = 10
    INDEX_BD_SNODE_SLEEP_TIME_0 = 11
    INDEX_BD_SNODE_SLEEP_TIME_1 = 12
    INDEX_BD_SNODE_SLEEP_TIME_2 = 13
    INDEX_BD_SNODE_SLEEP_TIME_3 = 14
    INDEX_BD_PATH_HC = 15
    INDEX_BD_PATH_RSSI = 16
    INDEX_BD_SNODE_JOIN_ID = 17 # Repeated
    INDEX_BD_SNODE_JOIN_RESPONSE = 18 # Repeated

    INDEX_DATA_ORIGINAL_ID = 0
    INDEX_DATA_ASSIGNED_ID = 1
    INDEX_DATA_PARENT_ID = 2
    INDEX_DATA_PARENT_RSSI = 3
    INDEX_DATA_BCAST_COUNT = 4
    INDEX_DATA_WTB_0 = 5
    INDEX_DATA_WTB_1 = 6
    INDEX_DATA_WTB_2 = 7
    INDEX_DATA_WTB_3 = 8
    INDEX_DATA_SEND_COUNT_0 = 9
    INDEX_DATA_SEND_COUNT_1 = 10
    INDEX_DATA_PREV_DATA_TRANS = 11
    INDEX_DATA_MISSED_MSGS = 12
    INDEX_DATA_MAX_QUEUE_SIZE = 13
    INDEX_DATA_CRC_FAILS = 14
    INDEX_DATA_FLAGS = 15
    INDEX_DATA_ANY_TRANSMISSIONS = 16
    INDEX_DATA_DROPPED_MSGS = 17
    INDEX_DATA_HC_DOWNSTREAM = 18
    INDEX_DATA_HC_UPSTREAM = 19
    INDEX_DATA_MISSED_BCASTS = 20

    ############################################################################
    # SEEL Parameters    
    SEEL_CYCLE_AWAKE_TIME_MILLIS = 60000
    SEEL_CYCLE_SLEEP_TIME_MILLIS = 60000
    
    SEEL_FORCE_SLEEP_AWAKE_MULT = 1.0
    SEEL_FORCE_SLEEP_AWAKE_DURATION_SCALE = 1.5
    SEEL_FORCE_SLEEP_RESET_COUNT = 3

############################################################################
# Default Parameters

parameters = Parameters()

############################################################################
# External Parameters

# Pull in overriding parameters from cmd line argument
# Pass in path to a file as the 2nd argument to this script
# File must have a "Parameters" class similar to the one in this script
# See the example file format in "misc/deployment_info/deployment_info.py"
# Format: python3 <path_to_this_file>/SEEL_log_parser.py <path_to_data_file>/<data_file> (optional)<path to param file>/<param file>

if len(sys.argv) > 2:
    sys.path.insert(1, os.path.dirname(sys.argv[2]))
    extern_file = __import__(os.path.basename(os.path.splitext(sys.argv[2])[0])) 
    Parameters_Mixed = type("Parameters_Mixed", (extern_file.Parameters, Parameters), {})
    parameters = Parameters_Mixed()

############################################################################
# Global Variables

bcast_times = []
bcast_info = []
bcast_inst_count = {} # Tracks number of previous bcasts at the start of a new bcast instance
bcast_info_overflow = {}
bcast_instances = {} # per node, since nodes may join at different times

node_mapping = {}
node_msgs = {}

node_analysis = {} # per node
msg_analysis = {} # per node

############################################################################

class Parent:
    def __init__(self, id, rssi):
        self.id = id
        self.rssi = rssi
     
class Cycle_Stats:
    def __init__(self):
        self.parent_rssi = 0
        self.crc_fails = -1
        self.data_transmissions = -1

class Bcast_Info:
    def __init__(self, bcast_num, bcast_inst, sys_time, awk_time, slp_time):
        self.bcast_num = bcast_num
        self.bcast_inst = bcast_inst
        self.sys_time = sys_time
        self.awk_time = awk_time
        self.slp_time = slp_time

    def __str__(self):
        return "Bcast num: " + str(self.bcast_num) + " Inst: " + str(self.bcast_inst) + " System time: " + str(self.sys_time) + " Awake time: " + str(self.awk_time) + " Sleep time: " + str(self.slp_time)
    
class Node_Msg:
    def __init__(self, bcast_num, bcast_inst, wtb, prev_data_trans, original_node_id, assigned_node_id, parent_id, parent_rssi, send_count, prev_queue_size, prev_missed_msgs, prev_missed_bcasts, prev_crc_fails, prev_flags, prev_any_trans, prev_dropped_msgs, hc_downstream, hc_upstream):
        self.bcast_num = bcast_num
        self.bcast_inst = bcast_inst
        self.wtb = wtb
        self.original_node_id = original_node_id
        self.assigned_node_id = assigned_node_id
        self.parent_id = parent_id
        self.parent_rssi = parent_rssi
        self.send_count = send_count
        self.prev_data_trans = prev_data_trans
        self.prev_queue_size = prev_queue_size
        self.prev_missed_msgs = prev_missed_msgs
        self.prev_missed_bcasts = prev_missed_bcasts
        self.prev_crc_fails = prev_crc_fails # Received CRC fails, not sent
        self.prev_flags = prev_flags
        self.prev_any_trans = prev_any_trans
        self.prev_dropped_msgs = prev_dropped_msgs
        self.hc_downstream = hc_downstream
        self.hc_upstream = hc_upstream

    def __str__(self):
        return "Bcast num: " + str(self.bcast_num) + "\tBcast inst: " + str(self.bcast_inst) + "\tNode ID: " + str(self.original_node_id) + \
        "\tParent ID: " + str(self.parent_id) + "\tParent RSSI: " + str(self.parent_rssi) + "\tSend Count: " + str(self.send_count) + \
        "\tDownstream HC: " + str(self.hc_downstream) + "\tUpstream HC: " + str(self.hc_upstream) + "\tWTB: " + str(self.wtb) + \
        "\tPrev Data Trans: " + str(self.prev_data_trans) + "\tPrev Any Trans: " + str(self.prev_any_trans) + "\tPrev Dropped Msgs: " + \
        str(self.prev_dropped_msgs) + "\tPrev Max Q Size: " + str(self.prev_queue_size) + "\tMissed Msgs: " + str(self.prev_missed_msgs) + \
        "\tMissed Bcasts: " + str(self.prev_missed_bcasts) + "\tPrev CRC Fails: " + str(self.prev_crc_fails) + "\tPrev Flags: " + \
        str( "{:08b}".format(self.prev_flags))

class Node_Analysis: # Per node
    def __init__(self):
        self.node_id = 0
        self.connections = {}
        self.connection_total = 0  
        self.paths = []
        self.cycle_children = {} # per cycle, 0 padded if no data. Index by bcast_inst and then bcast_num
        self.cycle_hops = {} # per cycle, 0 padded if no data. Index by bcast_inst and then bcast_num
        self.children_counts = {}
        self.children_rssi = {}
        self.hops = []
        self.hc_mean = 0
        self.children_mean = 0
        self.cycles = 0 # cycles alive for
        self.data_transmissions = []
        self.avg_data_transmissions = 0
        self.crc_fails = []
        self.avg_crc_fails = 0
        self.max_queue_sizes = []
        self.avg_max_queue_sizes = 0 # Max queue size avg'd across all received messages
        self.parent_cycle_rssi = {}
        self.rssi = []
        self.avg_rssi = 0
        self.highest_parent_ratio = 0 # Highest connection parent ratio [0, 1]
        self.avg_wtb = 0
        self.missed_msgs = [] # Format [Unique cycle, num_missed_msgs]
        self.avg_missed_msgs = 0
        self.flags = {}
        self.any_transmissions = []
        self.dropped_msgs = []
        self.hc_downstream = []
        self.hc_upstream = []

class Msg_Analysis: # Per node
    def __init__(self):
        self.node_id = 0
        self.cycle_stats = {} # per bcast inst, per bcast cycle

def node_entry(original_id, assigned_id, bcast_join):
    print("join id: " + str(original_id) + "\tresponse: " + str(assigned_id) + "\tB. Join: " + str(bcast_join))
    if assigned_id in node_mapping:
        print("WARNING: Assigned ID " + str(assigned_id) + " is assigned to multiple SNODEs")
    node_mapping[assigned_id] = original_id
    if not original_id in node_msgs:
        node_msgs[original_id] = []
        bcast_instances[original_id] = bcast_join

def search_paths(b_ind, b_num, node_analysis, paths, search_stack, hcount):        
    search_val = search_stack[-1]
    # DFS for path append
    for p in paths:
        if paths[p].id == search_val:
            search_stack.append(p)
            hcount += 1
            node_analysis[p].paths.append(search_stack[:])
            node_analysis[p].hops.append(hcount)
            if not b_ind in node_analysis[p].cycle_hops:
                node_analysis[p].cycle_hops[b_ind] = {}
            node_analysis[p].cycle_hops[b_ind][b_num] = hcount
            search_paths(b_ind, b_num, node_analysis, paths, search_stack, hcount)
            hcount -= 1
            search_stack.pop()

def plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label, regression=True, a=1):
        plt.scatter(x_ax, y_ax, alpha=a)
        if regression:
            try:
                lin_reg_model = np.polyfit(x_ax, y_ax, deg=1)
                xseq = np.linspace(min(x_ax), max(x_ax), num=100)
                plt.plot(xseq, lin_reg_model[1] + lin_reg_model[0] * xseq)
                model_predict = np.poly1d(lin_reg_model)
                r2 = r2_score(y_ax, model_predict(x_ax))
                plt.title(title + ", R^2=" + str(r2))
            except:
                print("Linear Regression failed")
                plt.title(title)
        else:
            plt.title(title)
        plt.xlabel(x_label)
        plt.ylabel(y_label)
        plt.show()

def read_as_int(list, index):
    if index < len(list):
        return int(list[index])
    else:
        return -9999

def main():
    if len(sys.argv) <= 1:
        print("Unspecified data file")
        exit()
    df_name = sys.argv[1]        

    #if len(sys.argv) > 2:
        #print(sys.argv[2])
        #info_module = __import__(sys.argv[2])
        #print(info_module)

    df = open(df_name)
    df_read = df.readlines()
    df_length = len(df_read)

    if len(parameters.HARDCODED_NODE_JOINS) > 0:
        print("Using HARDCODED Node Joins")
        for i in parameters.HARDCODED_NODE_JOINS:
            node_entry(i[parameters.HC_NJ_ORIGINAL_ID_IDX], i[parameters.HC_NJ_ASSIGNED_ID_IDX], i[parameters.HC_NJ_CYCLE_JOIN_IDX]);

    if len(parameters.HARDCODED_NODE_LOCS) > 0:
        print("Using HARDCODED Node Locs")
        # Uncomment and use for location normalization
        #for i in HARDCODED_NODE_LOCS:
            #print("[" + str(i) + ", " + str(round(HARDCODED_NODE_LOCS[i][0] - <NORM_X>, 7)) + ", " + str(round(HARDCODED_NODE_LOCS[i][1] - <NORM_Y>, 7)) + "]")

    if len(parameters.HARDCODED_PLOT_EXCLUDE) > 0:
        print("Using HARDCODED Plot Excludes")

    # Parse Logs
    current_line = 0
    bcast_instance = -1
    bcast_count = 0
    bcast_inst_count[0] = 0
    while current_line < df_length:
        line = df_read[current_line].split()
        if len(line) == 0:
            current_line += 1
            continue
        line[1:len(line)] = list(map(int, line[1:len(line)]))
        if line[parameters.INDEX_HEADER] == "BT:": # Bcast time
            bcast_times.append(line[parameters.INDEX_BT_TIME])
        elif line[parameters.INDEX_HEADER] == "BD:": # Bcast data
            sys_time = 0
            awk_time = 0
            slp_time = 0
            sys_time += read_as_int(line, parameters.INDEX_BD_SYS_TIME_0) << 24
            sys_time += read_as_int(line, parameters.INDEX_BD_SYS_TIME_1) << 16
            sys_time += read_as_int(line, parameters.INDEX_BD_SYS_TIME_2) << 8
            sys_time += read_as_int(line, parameters.INDEX_BD_SYS_TIME_3)
            awk_time += read_as_int(line, parameters.INDEX_BD_SNODE_AWAKE_TIME_0) << 24
            awk_time += read_as_int(line, parameters.INDEX_BD_SNODE_AWAKE_TIME_1) << 16
            awk_time += read_as_int(line, parameters.INDEX_BD_SNODE_AWAKE_TIME_2) << 8
            awk_time += read_as_int(line, parameters.INDEX_BD_SNODE_AWAKE_TIME_3)
            slp_time += read_as_int(line, parameters.INDEX_BD_SNODE_SLEEP_TIME_0) << 24
            slp_time += read_as_int(line, parameters.INDEX_BD_SNODE_SLEEP_TIME_1) << 16
            slp_time += read_as_int(line, parameters.INDEX_BD_SNODE_SLEEP_TIME_2) << 8
            slp_time += read_as_int(line, parameters.INDEX_BD_SNODE_SLEEP_TIME_3)
            if read_as_int(line, parameters.INDEX_BD_FIRST) > 0:
                bcast_instance += 1
                bcast_inst_count[bcast_instance] = bcast_count
            bcast_count += 1            
            bcast_info.append(Bcast_Info(read_as_int(line, parameters.INDEX_BD_BCAST_COUNT), bcast_instance, sys_time, awk_time, slp_time))
            
            for i in range(math.floor((len(line) - parameters.INDEX_BD_SNODE_JOIN_ID) / 2)):
                repeat_index = i * 2
                join_id = read_as_int(line, parameters.INDEX_BD_SNODE_JOIN_ID + repeat_index)
                if join_id != 0:
                    response = read_as_int(line, parameters.INDEX_BD_SNODE_JOIN_RESPONSE + repeat_index)
                    if response != 0: # Reponse of 0 means error
                        node_entry(join_id, response, len(bcast_times))
        else: # Node Data
            wtb = 0
            send_count = 0
            wtb += read_as_int(line, parameters.INDEX_DATA_WTB_0) << 24
            wtb += read_as_int(line, parameters.INDEX_DATA_WTB_1) << 16
            wtb += read_as_int(line, parameters.INDEX_DATA_WTB_2) << 8
            wtb += read_as_int(line, parameters.INDEX_DATA_WTB_3)
            send_count += read_as_int(line, parameters.INDEX_DATA_SEND_COUNT_0) << 8
            send_count += read_as_int(line, parameters.INDEX_DATA_SEND_COUNT_1)
            original_node_id = read_as_int(line, parameters.INDEX_DATA_ORIGINAL_ID)
            assigned_node_id = read_as_int(line, parameters.INDEX_DATA_ASSIGNED_ID)
            if not original_node_id in node_msgs:
                print("Picked up non-bcast ID: " + str(original_node_id))
                node_mapping[assigned_node_id] = original_node_id
                node_msgs[original_node_id] = []
                bcast_instances[original_node_id] = len(bcast_times)
            node_msgs[original_node_id].append(Node_Msg(read_as_int(line, parameters.INDEX_DATA_BCAST_COUNT), \
                bcast_instance, wtb, read_as_int(line, parameters.INDEX_DATA_PREV_DATA_TRANS), original_node_id, \
                assigned_node_id, read_as_int(line, parameters.INDEX_DATA_PARENT_ID), read_as_int(line, \
                parameters.INDEX_DATA_PARENT_RSSI) - 256, send_count, read_as_int(line, parameters.INDEX_DATA_MAX_QUEUE_SIZE), \
                read_as_int(line, parameters.INDEX_DATA_MISSED_MSGS), read_as_int(line, parameters.INDEX_DATA_MISSED_BCASTS), \
                read_as_int(line, parameters.INDEX_DATA_CRC_FAILS), read_as_int(line, parameters.INDEX_DATA_FLAGS), read_as_int(line, \
                parameters.INDEX_DATA_ANY_TRANSMISSIONS), read_as_int(line, parameters.INDEX_DATA_DROPPED_MSGS), read_as_int(line, \
                parameters.INDEX_DATA_HC_DOWNSTREAM), read_as_int(line, parameters.INDEX_DATA_HC_UPSTREAM)))
        current_line += 1

    # Analysis vars
    total_bcasts = len(bcast_times)
    node_mapping[0] = 0
    message_paths = {}
    
    # Plotting vars
    if parameters.PLOT_DISPLAY:
        if len(parameters.HARDCODED_NODE_LOCS) > 0:
            G = nx.DiGraph()
        if parameters.PLOT_RSSI_ANALYSIS:
            analysis_rssi = []
            analysis_transmissions = []

    # GNODE 
    print("Total Bcasts: " + str(total_bcasts))
    print("Initial Node mapping: " + str(node_mapping))
    
    # SNODE
    for node_msg_key in node_msgs:
        print("********************************************************")
        print("Node " + str(node_msg_key))
        node_data_msgs = node_msgs[node_msg_key]
        if not node_data_msgs: # Empty
            print("No messages received")
            continue
        duplicate_msg_tracker = {}
        wtb_general = []
        wtb_missed_bcast = []
        wtb_max_missed_bcast = []
        node_id = node_data_msgs[0].original_node_id
        num_node_msgs = len(node_data_msgs)
        total_data_transmissions = 0
        total_parent_rssi = 0
        total_rssi_counts = 0
        send_count_tracker = 0
        dropped_packets = 0
        dropped_packet_tracker = []
        duplicate_msg = 0
        connection_count = 0 # How many unique (bcast) data msgs we received
        connection_count_max = 0 # Max number of unique bcasts based on highest bcast count per bcast instance
        connection_count_overflow = 0 # Handles 256 overflow in a bcast instance
        connection_count_last_overflow = 0 # Enforces overflow rates so single late msg cannot cause another overflow
        connection_inst_set = set() # Tracks current bcast number per bcast instance, make set to remove dups
        connection_inst_max = 0 # Tracks current max bcast number per bcast instance
        connections = {}
        connections_rssi = {}
        queue_size_counter = 0
        max_queue_size = 0
        total_missed_msgs = {}
        total_missed_bcasts = {}
        total_crc_fails = 0
        first_wtb = True
        first_bcast_num = -1
        prev_bcast_num = -1
        prev_bcast_inst = -1
        total_bcasts_for_node = total_bcasts - bcast_instances[node_id] + 1

        # Analysis
        node_analysis[node_id] = Node_Analysis()
        node_analysis[node_id].node_id = node_id
        msg_analysis[node_id] = Msg_Analysis()
        msg_analysis[node_id].node_id = node_id

        # Plotting
        plot_snode_bcast_nums = []
        plot_snode_bcast_insts = []
        plot_snode_bcast_used = []

        if parameters.PLOT_RSSI_ANALYSIS:
            analysis_reset = True # Resets on first time or missed bcasts
            analysis_prev_data = [0, 0, 0]

        for msg in node_data_msgs:
            dup = True
            # Figure out how many messages we received from the SNODE versus how many we possibly could have received
            if msg.bcast_inst != prev_bcast_inst:
                if prev_bcast_inst >= 0:
                    connection_inst_max -= (0 if first_bcast_num < 0 else (first_bcast_num - 1)) # Check if bcast num didn't start at 0 in this inst
                    print("Received messages in bcast instance: " + str(len(connection_inst_set)) + "/" + str(connection_inst_max), str(0 if connection_inst_max == 0 else len(connection_inst_set) / connection_inst_max))        
                    connection_count += len(connection_inst_set)
                    connection_count_max += connection_inst_max
                prev_bcast_inst = msg.bcast_inst
                connection_count_overflow = 0
                connection_count_last_overflow = 0
                connection_inst_set = set()
                connection_inst_max = 0
                prev_bcast_num = -1
                first_bcast_num = -1

            if (len(connection_inst_set) > parameters.PARAM_COUNT_WRAP_SAFETY or msg.bcast_num < (parameters.PARAM_COUNT_WRAP_SAFETY if len(connection_inst_set) == 0 else max(connection_inst_set)+ parameters.PARAM_COUNT_WRAP_SAFETY)): # Not from previous bcast inst
                if msg.bcast_num < prev_bcast_num and prev_bcast_num > 192 and msg.bcast_num < 64 and len(connection_inst_set) > (connection_count_last_overflow + parameters.PARAM_COUNT_WRAP_SAFETY): # Assume bcast num overflowed, values used are 3/4 of 256 and 1/4 of 256
                    #print("Debug: overflow")
                    connection_count_overflow += 256
                    connection_count_last_overflow = len(connection_inst_set)
                
                overflow_comp_bcast_num = msg.bcast_num + connection_count_overflow
                # Message from non-overflow case came late
                if msg.bcast_num > (255 - parameters.PARAM_COUNT_WRAP_SAFETY) and (len(connection_inst_set) - connection_count_last_overflow) < parameters.PARAM_COUNT_WRAP_SAFETY:
                    overflow_comp_bcast_num -= 256;
                
                if not overflow_comp_bcast_num in connection_inst_set: # Dup check
                    #print("DEBUG: add " + str(overflow_comp_bcast_num))
                    connection_inst_set.add(overflow_comp_bcast_num)
                    #print("DEBUG: connection_inst_max -> " + str(connection_inst_max))
                    if overflow_comp_bcast_num > connection_inst_max:
                        connection_inst_max = overflow_comp_bcast_num
                    if first_bcast_num < 0:
                        first_bcast_num = msg.bcast_num
                    prev_bcast_num = msg.bcast_num
                    plot_snode_bcast_nums.append(msg.bcast_num)
                    plot_snode_bcast_insts.append(msg.bcast_inst)
                    plot_snode_bcast_used.append(False)
                    dup = False
                    if parameters.PRINT_ALL_MSGS:
                        print(str(msg)) 
                #else:
                    #print("Debug: dup")
            #else:
                #print(str(msg)) 
                #print("Debug: ignore")

            if not dup:
                # Unique bcast number across all bcast instances
                node_unique_cycle_num = bcast_inst_count[msg.bcast_inst] + overflow_comp_bcast_num - (0 if first_bcast_num < 0 else first_bcast_num)
                #print("Debug: node unique bcast" + str(node_unique_cycle_num))
            
                # Update node mappings with latest ID since they could change throughout deployment
                # However, this should NOT occur since assigned IDs should be UNIQUE
                # In previous deployments, there was a bug that caused assigned IDs to be non-unique which is why this code is needed
                if node_mapping[msg.assigned_node_id] != msg.original_node_id:
                    print("Updated node mapping of assigned ID " + str(msg.assigned_node_id) + " from " + str(node_mapping[msg.assigned_node_id]) + " to " + str(msg.original_node_id))
                    node_mapping[msg.assigned_node_id] = msg.original_node_id
            
                if not msg.parent_id in node_mapping:
                    print("Invalid parent ID: " + str(msg.parent_id), flush=True)
                    exit()
                parent_original_id = node_mapping[msg.parent_id]
                
                if not msg.bcast_inst in bcast_info_overflow:
                    bcast_info_overflow[msg.bcast_inst] = overflow_comp_bcast_num
                else:
                    bcast_info_overflow[msg.bcast_inst] = (overflow_comp_bcast_num if overflow_comp_bcast_num > bcast_info_overflow[msg.bcast_inst] else bcast_info_overflow[msg.bcast_inst]) # Insert largest bcast_info_overflow value for a bcast_inst
            
                # remove previously missed packet if the packet came in late
                if dropped_packet_tracker.count(msg.send_count) > 0:
                    dropped_packet_tracker.remove(msg.send_count)
                    dropped_packets -= 1

                for i in range(send_count_tracker + 1, msg.send_count):
                    dropped_packets += 1
                    if dropped_packet_tracker.count(i) == 0:
                        dropped_packet_tracker.append(i)
                send_count_tracker = msg.send_count

                # Ignore first WTB during analysis since not system sync'd yet
                if not first_wtb:
                    wtb_general.append(msg.wtb)
                    # After SEEL_FORCE_SLEEP_RESET_COUNT missed bcasts, WTB factors the entire missed cycle since the 
                    # device stays awake the entire cycle. Separately track these values to see how impactful they are.
                    if msg.prev_missed_msgs > 0: 
                        if msg.prev_missed_msgs < parameters.SEEL_FORCE_SLEEP_RESET_COUNT:
                            # Scalar should be (AWAKE_MULT * DUR_SCALE ^ 1 - 1) + (AWAKE_MULT * DUR_SCALE ^ 2 - 1) + ... + (AWAKE_MULT * DUR_SCALE ^ MISSED_BCASTS - 1)
                            # Which reduces to the following form
                            awake_time_scalar = np.sum(np.power(parameters.SEEL_FORCE_SLEEP_AWAKE_DURATION_SCALE, np.arange(1, msg.prev_missed_msgs + 1))) * parameters.SEEL_FORCE_SLEEP_AWAKE_MULT - msg.prev_missed_msgs
                            extra_awake_time_millis = parameters.SEEL_CYCLE_AWAKE_TIME_MILLIS * awake_time_scalar
                            wtb_missed_bcast.append(extra_awake_time_millis)
                        elif msg.prev_missed_msgs == parameters.SEEL_FORCE_SLEEP_RESET_COUNT:
                            wtb_missed_bcast.append(msg.wtb)
                            wtb_max_missed_bcast.append(msg.wtb)
                else:
                    first_wtb = False

                # Not all sent msgs are received, "prev_data_trans" indicates how many total data transmissions 
                # were done by the node in the previous cycle. Duplicates within a cycle can exist
                total_data_transmissions += msg.prev_data_trans
                total_crc_fails += msg.prev_crc_fails

                if msg.parent_rssi != -256: # Impossible value, used to flag RSSI unavailable
                    total_parent_rssi += msg.parent_rssi
                    if not parent_original_id in node_analysis[node_id].parent_cycle_rssi:
                        node_analysis[node_id].parent_cycle_rssi[parent_original_id] = []
                    bcast_rssi_pair = [node_unique_cycle_num, msg.parent_rssi]
                    node_analysis[node_id].parent_cycle_rssi[parent_original_id].append(bcast_rssi_pair)
                
                if not parent_original_id in connections:
                    connections[parent_original_id] = []
                    connections_rssi[parent_original_id] = []
                connections[parent_original_id].append(node_unique_cycle_num)
                connections_rssi[parent_original_id].append(msg.parent_rssi)
                
                # Create a structure that contains all the paths in the network per bcast num separated by bcast inst
                if not msg.bcast_inst in message_paths:
                    message_paths[msg.bcast_inst] = {}
                if not overflow_comp_bcast_num in message_paths[msg.bcast_inst]:
                    message_paths[msg.bcast_inst][overflow_comp_bcast_num] = {}
                message_paths[msg.bcast_inst][overflow_comp_bcast_num][msg.original_node_id] = Parent(parent_original_id, msg.parent_rssi)

                queue_size_counter += msg.prev_queue_size
                if msg.prev_queue_size > max_queue_size:
                    max_queue_size = msg.prev_queue_size

                if msg.prev_missed_msgs in total_missed_msgs:
                    total_missed_msgs[msg.prev_missed_msgs] += 1
                else:
                    total_missed_msgs[msg.prev_missed_msgs] = 1

                if msg.prev_missed_bcasts in total_missed_bcasts:
                    total_missed_bcasts[msg.prev_missed_bcasts] += 1
                else:
                    total_missed_bcasts[msg.prev_missed_bcasts] = 1
        
                node_analysis[node_id].data_transmissions.append(msg.prev_data_trans)
                node_analysis[node_id].crc_fails.append(msg.prev_crc_fails)
                node_analysis[node_id].max_queue_sizes.append(msg.prev_queue_size)
                node_analysis[node_id].rssi.append(msg.parent_rssi)
                node_analysis[node_id].missed_msgs.append([node_unique_cycle_num, msg.prev_missed_msgs])
                node_analysis[node_id].any_transmissions.append(msg.prev_any_trans)
                node_analysis[node_id].dropped_msgs.append(msg.prev_dropped_msgs)
                node_analysis[node_id].hc_downstream.append(msg.hc_downstream)
                node_analysis[node_id].hc_upstream.append(msg.hc_upstream)
        
                if not msg.prev_flags in node_analysis[node_id].flags:
                    node_analysis[node_id].flags[msg.prev_flags] = 0
                node_analysis[node_id].flags[msg.prev_flags] += 1
        
                if not msg.bcast_inst in msg_analysis[node_id].cycle_stats:
                    msg_analysis[node_id].cycle_stats[msg.bcast_inst] = {}
                if not msg.bcast_num in msg_analysis[node_id].cycle_stats[msg.bcast_inst]:
                    msg_analysis[node_id].cycle_stats[msg.bcast_inst][msg.bcast_num] = Cycle_Stats()
                msg_analysis[node_id].cycle_stats[msg.bcast_inst][msg.bcast_num].parent_rssi = msg.parent_rssi
                # Prev cycle msgs
                if not (msg.bcast_num - 1) in msg_analysis[node_id].cycle_stats[msg.bcast_inst]:
                    msg_analysis[node_id].cycle_stats[msg.bcast_inst][msg.bcast_num - 1] = Cycle_Stats()
                msg_analysis[node_id].cycle_stats[msg.bcast_inst][msg.bcast_num - 1].data_transmissions = msg.prev_data_trans
                msg_analysis[node_id].cycle_stats[msg.bcast_inst][msg.bcast_num - 1].crc_fails = msg.prev_crc_fails

                if parameters.PLOT_RSSI_ANALYSIS:
                    # Compare against queue size 0 because otherwise we don't know how many msgs we got this cycle
                    if analysis_reset or msg.prev_data_trans == 0 or msg.prev_queue_size != 0 or msg.parent_id != 0 or msg.bcast_num != (analysis_prev_data[0] + 1):
                        analysis_prev_data = [msg.bcast_num, msg.parent_rssi, msg.prev_queue_size]
                        analysis_reset = False
                    else:
                        tpm = msg.prev_data_trans / (analysis_prev_data[2] - msg.prev_queue_size + 1); # Average transmissions per msg

                        analysis_rssi.append(analysis_prev_data[1]);
                        analysis_transmissions.append(tpm);

                        analysis_prev_data = [msg.bcast_num, msg.parent_rssi, msg.prev_queue_size]
                        analysis_reset = False
            else: # if dup
                duplicate_msg += 1
        connection_inst_max -= (0 if first_bcast_num < 0 else (first_bcast_num - 1)) 
        print("Received messages in bcast instance: " + str(len(connection_inst_set)) + "/" + str(connection_inst_max), str(0 if connection_inst_max == 0 else 
        len(connection_inst_set) / connection_inst_max))
        # Node lifetime is the current connection instances plus the previous max instances
        node_analysis[node_id].cycles = connection_count_max + len(connection_inst_set)
        print("Estimated node lifetime (cycles): " + str(node_analysis[node_id].cycles))
        connection_count += len(connection_inst_set)
        connection_count_max += connection_inst_max
        
        # Store and Print Analysis
        print("\tJoined Network on Bcast: " + str(bcast_instances[node_id]))
        print("\tTotal Received Messages: " + str(num_node_msgs))
        print("\tDuplicate Messages: " + str(duplicate_msg))
        if (num_node_msgs == duplicate_msg):
            print("Not enough data")
            continue
        print("\tPDR Calculations")
        print("\t\tDropped Packets: " + str(connection_count_max - connection_count))
        print("\t\tPercentage over GNODE lifetime: " + str(connection_count / total_bcasts_for_node)) # Total number of GNODE Bcasts received
        node_analysis[node_id].PDR = (0 if connection_count_max == 0 else connection_count / connection_count_max) # Total given times connected and generated a msg (until disconnect or death). This metric is a better representation of PDR.
        print("\t\tPDR_NO_MM (No MM Adjustment): " + str(node_analysis[node_id].PDR))
        connection_count_adjustment = 0
        for mm_key in total_missed_msgs:
            connection_count_adjustment += mm_key * total_missed_msgs[mm_key] # Warning: This will ignore bcasts in packets with more than the max missed msgs count
        try:
            pdr_mm_adjustment = (0 if connection_count_max == 0 else connection_count / (connection_count_max - connection_count_adjustment)) # Adjusts missed msgs, where a packet was NOT created so it should not affect PDR
            print("\t\tPDR (Missed Msg Adjustment): " + str(pdr_mm_adjustment))
            node_analysis[node_id].PDR = pdr_mm_adjustment
        except:
            print("\t\tPDR (Missed Msg Adjustment): Not Available")
        print("\tWTB")
        if len(wtb_general) > 0:
            node_analysis[node_id].avg_wtb = statistics.mean(wtb_general)
            print("\t\tMean General WTB Millis: " + str(node_analysis[node_id].avg_wtb))
            print("\t\tMedian General WTB Millis: " + str(statistics.median(wtb_general)))
            print("\t\tStd Dev. General WTB Millis: " + str(statistics.stdev(wtb_general)))
            total_wtb_general = node_analysis[node_id].avg_wtb * len(wtb_general)
            print("\t\tTotal General WTB Millis: " + str(total_wtb_general))
            print("\t\tGeneral WTB Instances: " + str(len(wtb_general)))
            print("\t\tComparing General WTB to ANY MISS WTB (a node missing > 0 number of bcasts)")
            if len(wtb_missed_bcast) > 0:
                mean_wtb_missed_bcast = statistics.mean(wtb_missed_bcast)
                print("\t\t\tMean ANY MISS WTB Millis: " + str(mean_wtb_missed_bcast))
                print("\t\t\tANY MISS WTB Instances: " + str(len(wtb_missed_bcast)))
                MM_adjusted_WTB = (total_wtb_general - (mean_wtb_missed_bcast * len(wtb_missed_bcast))) / (len(wtb_general) - len(wtb_missed_bcast))
                print("\t\t\tMean Drift WTB (General WTB - ANY MISS WTB) MILLIS: " + str(MM_adjusted_WTB))
            else:
                print("\t\t\tNo MISSES")
            print("\t\tComparing General WTB to MAX MISS WTB (a node missing SEEL_FORCE_SLEEP_RESET_COUNT number of bcasts)")
            if len(wtb_max_missed_bcast) > 0:
                mean_wtb_mass_missed_bcast = statistics.mean(wtb_max_missed_bcast)
                print("\t\t\tMean MAX MISS WTB Millis: " + str(mean_wtb_mass_missed_bcast))
                print("\t\t\tMAX MISS WTB Instances: " + str(len(wtb_max_missed_bcast)))
                MM_adjusted_WTB = (total_wtb_general - (mean_wtb_mass_missed_bcast * len(wtb_max_missed_bcast))) / (len(wtb_general) - len(wtb_max_missed_bcast))
                print("\t\t\tMean (General WTB - MAX MISS WTB) MILLIS: " + str(MM_adjusted_WTB))
            else:
                print("\t\t\tNo MAX MISSES")
        else:
            print("\t\tNo WTB info available")
        print("\tAvg Downstream HC: " + str(statistics.mean(node_analysis[node_id].hc_downstream)))
        print("\tAvg Upstream HC: " + str(statistics.mean(node_analysis[node_id].hc_upstream)))
        node_analysis[node_id].avg_data_transmissions = statistics.mean(node_analysis[node_id].data_transmissions)
        print("\tAvg Data Transmissions: " + str(node_analysis[node_id].avg_data_transmissions))        
        print("\tAvg Transmissions (Any type): " + str(statistics.mean(node_analysis[node_id].any_transmissions)))
        print("\tAvg Dropped Msgs: " + str(statistics.mean(node_analysis[node_id].dropped_msgs)))
        node_analysis[node_id].avg_max_queue_sizes = statistics.mean(node_analysis[node_id].max_queue_sizes)
        print("\tAvg Data Queue Size: " + str(node_analysis[node_id].avg_max_queue_sizes))
        print("\tMax Data Queue Size: " + str(max_queue_size))
        node_analysis[node_id].avg_CRC_fails = statistics.mean(node_analysis[node_id].crc_fails)
        print("\tAvg CRC received fails: " + str(node_analysis[node_id].avg_CRC_fails))
        print("\tMissed Msgs: " + str(total_missed_msgs))
        print("\tMissed Bcasts: " + str(total_missed_bcasts))
        print("\tFlags: " + str(node_analysis[node_id].flags))
        if len(parameters.HARDCODED_NODE_TDMA) > 0:
            print("\tParent Connections (TDMA): ")
        else:
            print("\tParent Connections: ")
        node_analysis[node_id].connection_total = sum([len(x) for x in connections.values()])
        node_analysis[node_id].highest_parent_ratio = 0
        for p_key in connections:
            total_connections = len(connections[p_key])
            if len(parameters.HARDCODED_NODE_TDMA) > 0:
                print("\t\t" + str(p_key) + " (" + str(parameters.HARDCODED_NODE_TDMA[p_key]) + ")" + ":\t" + str(total_connections))
            else:
                print("\t\t" + str(p_key) + ":\t" + str(total_connections))
            if total_connections > 0:
                parent_connection_ratio = total_connections / node_analysis[node_id].connection_total
                if parent_connection_ratio > node_analysis[node_id].highest_parent_ratio:
                    node_analysis[node_id].highest_parent_ratio  = parent_connection_ratio
                print("\t\t\tAvg RSSI: " + str(sum(connections_rssi[p_key]) / total_connections))
                if parameters.PLOT_DISPLAY and len(parameters.HARDCODED_NODE_LOCS) > 0:
                    G.add_edge(node_id, p_key, weight=total_connections/parameters.PLOT_LOCS_WEIGHT_SCALAR)
                node_analysis[node_id].connections[p_key] = total_connections
        node_analysis[node_id].avg_rssi = statistics.mean(node_analysis[node_id].rssi)
        node_analysis[node_id].avg_missed_msgs = statistics.mean([x[1] for x in node_analysis[node_id].missed_msgs])
        print(flush=True)
        
        # Node specific plots
        if parameters.PLOT_DISPLAY and parameters.PLOT_NODE_SPECIFIC_BCASTS:
            """ Bcasts Received Start """
            plot_snode_bcast_nums_padded = []
            if len(plot_snode_bcast_nums) > 0:
                # Since SNODE may have missed GNODE bcasts, fill SNODE array with same value (graph shows horizontal line) if missed
                plot_snode_bcast_nums_padded = []
                n_s_count = 0
                for n_g_idx in range(len(bcast_info)): # Start tracking when SNODE joined the network
                    n_g = bcast_info[n_g_idx].bcast_num
                    n_g_inst = bcast_info[n_g_idx].bcast_inst
                    #print("Debug: looking for " + str(n_g))
                    window_min = max(n_s_count - parameters.PARAM_COUNT_WRAP_SAFETY, 0)
                    window_max = min(n_s_count + parameters.PARAM_COUNT_WRAP_SAFETY, len(plot_snode_bcast_nums) - 1)
                    #print("Debug: Window min idx: " + str(window_min) + " Current idx: " + str(min(n_s_count, window_max)) + " Window max idx: " + str(window_max))
                    #print("Debug: Window min: " + str(plot_snode_bcast_nums[window_min]) + " Current: " + str(plot_snode_bcast_nums[min(n_s_count, window_max)]) + " Window max: " + str(plot_snode_bcast_nums[window_max]))
                    found = -1
                    count_inc = window_min - n_s_count
                    for n_s_idx in range(window_min, window_max):
                        n_s = plot_snode_bcast_nums[n_s_idx]
                        n_s_inst = plot_snode_bcast_insts[n_s_idx]
                        #print("\tDebug: looking at " + str(n_s))
                        count_inc += 1
                        if n_s == n_g and n_s_inst == n_g_inst and not plot_snode_bcast_used[n_s_idx]:
                            found = n_s
                            plot_snode_bcast_used[n_s_idx] = True
                            break
                    if found >= 0:
                        plot_snode_bcast_nums_padded.append(n_s)
                        n_s_count += count_inc
                    else:
                        if len(plot_snode_bcast_nums_padded) > 0:
                            plot_snode_bcast_nums_padded.append(0)
                            #print("\tDebug: padding " + str(plot_snode_bcast_nums_padded[-1]))
                        else:
                            plot_snode_bcast_nums_padded.append(0)
                            #print("\tDebug: padding " + str(0))
                    
                mm_x = [] # Bcast num
                mm_y = [] # 0
                for mm in node_analysis[node_id].missed_msgs:
                    mm_bcast = mm[0]
                    mm_count = mm[1]
                    for c in range(mm_count):
                        mm_x.append(mm_bcast - c - 1)
                        mm_y.append(0)
                figure, axis = plt.subplots(2, sharex=True)
                plt.title("Bcast Num vs Cycle")
                # GNODE
                axis[0].plot([bi.bcast_num for bi in bcast_info])
                axis[0].set_title("GNODE")
                axis[0].set_ylabel("Bcast Num")
                # SNODE
                axis[1].plot(plot_snode_bcast_nums_padded)
                axis[1].scatter(mm_x, mm_y, color="Red")
                axis[1].set_title("SNODE " + str(node_id))
                axis[1].set_xlabel("Cycle Num (Red dots are missed bcasts)")
                axis[1].set_ylabel("Bcast Num")
                plt.show()
                """ Bcasts Received End """

    # Data preparation
    # Build path structurea
    for b_ind in message_paths:
        for b_num in message_paths[b_ind]:
            paths = message_paths[b_ind][b_num]
            search_stack = [0]
            hcount = 0
            search_paths(b_ind, b_num, node_analysis, paths, search_stack, hcount) # Recursively adds in paths to nodes
            # Find and assign children
            for child in paths:
                parent = paths[child]
                if parent.id != 0: # node_analysis does not have data for GNODE (ID 0)
                    if not b_ind in node_analysis[parent.id].cycle_children:
                        node_analysis[parent.id].cycle_children[b_ind] = {}
                    if not b_num in node_analysis[parent.id].cycle_children[b_ind]:
                        node_analysis[parent.id].cycle_children[b_ind][b_num] = []
                    node_analysis[parent.id].cycle_children[b_ind][b_num].append(child)
                    if not child in node_analysis[parent.id].children_counts:
                        node_analysis[parent.id].children_counts[child] = 1
                        node_analysis[parent.id].children_rssi[child] = parent.rssi
                    else:
                        node_analysis[parent.id].children_counts[child] += 1
                        node_analysis[parent.id].children_rssi[child] += parent.rssi
    
    # Average children rssi
    for node in node_analysis.values():
        for child in node.children_counts:
            node.children_rssi[child] /= node.children_counts[child]
        
    # General Analysis
    print("Holistic Analysis")
    print("********************************************************")
    print("********************************************************")
    for node in node_analysis.values():
        print("********************************************************")
        print("Node " + str(node.node_id))
        print("Cycle Hopcount")
        if len(node.hops) > 0:
            print("\tData Points: " + str(len(node.hops)))
            node.hc_mean = statistics.mean(node.hops) # Save for use later
            print("\tMean: " + str(node.hc_mean))
            print("\tStd Dev: " + str(statistics.stdev(node.hops)))
        else:
            print("\tNot enough data")
        print("Cycle Children")
        TDMA_conflicts = []
        if len(node.cycle_children) > 0:
            children = []
            for b_inst in node.cycle_children:
                for b_num in node.cycle_children[b_inst]:
                    children.append(len(node.cycle_children[b_inst][b_num]))
                    if len(parameters.HARDCODED_NODE_TDMA) > 0:
                        cycle_TMDA_slots = set()
                        for child in node.cycle_children[b_inst][b_num]:
                            child_TDMA_slot = parameters.HARDCODED_NODE_TDMA[child]
                            dups = []
                            if not child_TDMA_slot in cycle_TMDA_slots:
                                for c in node.cycle_children[b_inst][b_num]:
                                    c_TDMA_slot = parameters.HARDCODED_NODE_TDMA[c]
                                    if c_TDMA_slot == child_TDMA_slot:
                                        dups.append(c)
                                cycle_TMDA_slots.add(child_TDMA_slot)
                            if len(dups) > 1:
                                TDMA_conflicts.append(dups)
            node.children_mean = statistics.mean(children) # Save for use later
            print("\tMean: " + str(node.children_mean))
            print("\tStd Dev: " + str(statistics.stdev(children)))
            if len(parameters.HARDCODED_NODE_TDMA) > 0:
                print("\tTDMA Conflicts: " + str(len(TDMA_conflicts)))
                print("\t\t" + str(TDMA_conflicts))
        else:
            print("\tNot enough data")
        
        if len(parameters.HARDCODED_NODE_TDMA) > 0:
            print("Children Connections (TDMA)")
        else:
            print("Children Connections")
        for child in node.children_counts:
            if len(parameters.HARDCODED_NODE_TDMA) > 0:
                print("\t" + str(child) + " (" + str(parameters.HARDCODED_NODE_TDMA[child]) + ")" + ":\t" + str(node.children_counts[child]))
            else:
                print("\t" + str(child) + ":\t" + str(node.children_counts[child]))
            print("\t\tAvg RSSI: " + str(node.children_rssi[child]))
    print(flush=True)

    # Plots
    if parameters.PLOT_DISPLAY:
        # Store PDR, prepares map for use below in append plot data
        node_PDR = {}
        node_PDR[0] = 1 # Give GNODE a PDR of 1
        for n_key in node_analysis:
            node_PDR[n_key] = node_analysis[n_key].PDR
    
        # Append node plot data
        node_avg_HC = []
        node_avg_children = []
        node_lifetime_cycles = []
        node_self_PDR = []
        node_weighted_parent_PDR = []
        node_avg_crc_receive_fails = []
        node_avg_data_transmissions = []
        node_avg_max_queue_sizes = []
        node_avg_rssi = []
        node_highest_parent_ratio = []
        node_avg_WTB = []
        node_avg_missed_msgs = []
        for n_key in node_analysis:
            node = node_analysis[n_key]
            if n_key in parameters.HARDCODED_PLOT_EXCLUDE:
                continue # Skip any nodes we mark as "exclude", such as outliers seen from previous runs
            node_self_PDR.append(node.PDR)
            node_avg_HC.append(node.hc_mean)
            node_avg_children.append(node.children_mean)
            node_lifetime_cycles.append(node.cycles)
            node_avg_crc_receive_fails.append(node.avg_CRC_fails)
            node_avg_data_transmissions.append(node.avg_data_transmissions)
            node_avg_max_queue_sizes.append(node.avg_max_queue_sizes)
            node_highest_parent_ratio.append(node.highest_parent_ratio)
            node_avg_rssi.append(node.avg_rssi)
            node_avg_WTB.append(node.avg_wtb)
            node_avg_missed_msgs.append(node.avg_missed_msgs)
            total_parents = len(node.connections)
            total_parent_PDR = 0
            total_parent_connections = 0
            for parent in node.connections:
                total_parent_PDR += node_PDR[parent] * node.connections[parent]
                total_parent_connections += node.connections[parent]
            average_parent_PDR = total_parent_PDR / total_parent_connections
            node_weighted_parent_PDR.append(average_parent_PDR)
        
        # Append msg plot data
        msg_parent_rssi = []
        msg_crc_fails = []
        msg_data_transmissions = []
        msg_dropped_cycles = 0
        for msg in msg_analysis.values():
            for bi in msg.cycle_stats:
                for bn in msg.cycle_stats[bi]:
                    cs = msg.cycle_stats[bi][bn] # cycles stats
                    # Only take Cycle Stats with entire stats filled out
                    if cs.parent_rssi != 0 and cs.data_transmissions != -1 and cs.crc_fails != -1:
                        msg_parent_rssi.append(cs.parent_rssi)
                        msg_data_transmissions.append(cs.data_transmissions)
                        msg_crc_fails.append(cs.crc_fails)
                    else:
                        msg_dropped_cycles += 1
        print("Plot Msg total good data: " + str(len(msg_parent_rssi)))
        print("Plot Msg total bad data: " + str(msg_dropped_cycles)) 
        
        # *********** SPECIFIC ANALYSIS PLOTS ***********
        
        if parameters.PLOT_RSSI_ANALYSIS:
            print("RSSI Analysis # data points: " + str(len(analysis_rssi)), flush=True)
            plt.scatter(analysis_rssi, analysis_transmissions, alpha=0.1);
            plt.title("Transmissions vs RSSI")
            plt.xlabel("RSSI")
            plt.ylabel("Transmissions")
            plt.show()
            
        # Plot each parent-child combination's RSSI's over all cycles
        if parameters.PLOT_NODE_PARENT_CYCLE_RSSI:
            for n_key in node_analysis:
                node = node_analysis[n_key]
                for p_key in node.parent_cycle_rssi:
                    # print(node.parent_cycle_rssi[p_key], flush=True)
                    unique_cycle_num = []
                    connection_RSSI = []
                    for cycle in node.parent_cycle_rssi[p_key]:
                        unique_cycle_num.append(cycle[0])
                        connection_RSSI.append(cycle[1])
                    plt.scatter(unique_cycle_num, connection_RSSI);
                    plt.title("Node " + str(n_key) + " to Parent " + str(p_key) + " RSSI")
                    plt.xlabel("Cycle")
                    plt.ylabel("RSSI")
                    plt.xlim([0, len(bcast_times)])
                    plt.ylim([min(-120, min(connection_RSSI)), max(-50, max(connection_RSSI))])
                    plt.show()
                    
        
        # *********** NODE PLOTS ***********
        
        # Self PDR vs Weighted (Connections) Parent PDR
        x_ax = node_weighted_parent_PDR
        y_ax = node_self_PDR
        title = "NODE: Self PDR vs Weighted Parent PDR"
        x_label = "Weighted Parent PDR"
        y_label = "Self PDR"
        for i, node_id in enumerate([node_analysis[n_key].node_id for n_key in node_analysis if not n_key in parameters.HARDCODED_PLOT_EXCLUDE]):
            plt.annotate(node_id, (x_ax[i], y_ax[i]))
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label)
    
        # PDR vs Avg HC
        x_ax = node_avg_HC
        y_ax = node_self_PDR
        title = "NODE: PDR vs Avg HC"
        x_label = "Avg HC"
        y_label = "PDR"
        for i, node_id in enumerate([node_analysis[n_key].node_id for n_key in node_analysis if not n_key in parameters.HARDCODED_PLOT_EXCLUDE]):
            plt.annotate(node_id, (x_ax[i], y_ax[i]))
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label)
    
        # PDR vs Avg Children
        x_ax = node_avg_children
        y_ax = node_self_PDR
        title = "NODE: PDR vs Avg Children"
        x_label = "Avg Children"
        y_label = "PDR"
        for i, node_id in enumerate([node_analysis[n_key].node_id for n_key in node_analysis if not n_key in parameters.HARDCODED_PLOT_EXCLUDE]):
            plt.annotate(node_id, (x_ax[i], y_ax[i]))
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label)
    
        # PDR vs CRC Fails
        x_ax = node_avg_crc_receive_fails
        y_ax = node_self_PDR
        title = "NODE: PDR vs CRC Fails"
        x_label = "CRC Fails"
        y_label = "PDR"
        for i, node_id in enumerate([node_analysis[n_key].node_id for n_key in node_analysis if not n_key in parameters.HARDCODED_PLOT_EXCLUDE]):
            plt.annotate(node_id, (x_ax[i], y_ax[i]))
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label)
        
        # PDR vs Avg Data Transmissions
        x_ax = node_avg_data_transmissions
        y_ax = node_self_PDR
        title = "NODE: PDR vs Avg Data Transmissions"
        x_label = "Avg Data Transmissions"
        y_label = "PDR"
        for i, node_id in enumerate([node_analysis[n_key].node_id for n_key in node_analysis if not n_key in parameters.HARDCODED_PLOT_EXCLUDE]):
            plt.annotate(node_id, (x_ax[i], y_ax[i]))
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label)
        
        # PDR vs Avg Max Queue Sizes 
        # Naming here is difficult; since queue sizes are reported as the max during a cycle, we take the avg of max queue sizes across received msgs
        x_ax = node_avg_max_queue_sizes
        y_ax = node_self_PDR
        title = "NODE: PDR vs Avg Queue Size"
        x_label = "Avg Queue Size"
        y_label = "PDR"
        for i, node_id in enumerate([node_analysis[n_key].node_id for n_key in node_analysis if not n_key in parameters.HARDCODED_PLOT_EXCLUDE]):
            plt.annotate(node_id, (x_ax[i], y_ax[i]))
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label)
    
        # PDR vs Highest parent ratio, highest parent ratio is the highest connection % a node had to one of its parents
        x_ax = node_highest_parent_ratio
        y_ax = node_self_PDR
        title = "NODE: PDR vs Highest Parent Ratio"
        x_label = "Highest Parent Ratio"
        y_label = "PDR"
        for i, node_id in enumerate([node_analysis[n_key].node_id for n_key in node_analysis if not n_key in parameters.HARDCODED_PLOT_EXCLUDE]):
            plt.annotate(node_id, (x_ax[i], y_ax[i]))
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label)
    
        # Avg Max Queue Size vs Avg Data Transmissions
        x_ax = node_avg_data_transmissions
        y_ax = node_avg_max_queue_sizes
        title = "NODE: Avg Max Queue Size vs Avg Data Transmissions"
        x_label = "Avg Data Transmissions"
        y_label = "Avg Max Queue Size"
        for i, node_id in enumerate([node_analysis[n_key].node_id for n_key in node_analysis if not n_key in parameters.HARDCODED_PLOT_EXCLUDE]):
            plt.annotate(node_id, (x_ax[i], y_ax[i]))
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label)
    
        # Avg Data Transmissions vs Avg RSSI
        x_ax = node_avg_rssi
        y_ax = node_avg_data_transmissions
        title = "NODE: Avg Data Transmissionsvs vs Avg RSSI"
        x_label = "Avg RSSI"
        y_label = "Avg Data Transmissions"
        for i, node_id in enumerate([node_analysis[n_key].node_id for n_key in node_analysis if not n_key in parameters.HARDCODED_PLOT_EXCLUDE]):
            plt.annotate(node_id, (x_ax[i], y_ax[i]))
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label)
    
        # Avg Missed Bcasts vs Avg RSSI
        x_ax = node_avg_rssi
        y_ax = node_avg_missed_msgs
        title = "NODE: Avg Missed Msgs vs Avg RSSI"
        x_label = "Avg RSSI"
        y_label = "Avg Missed Msgs"
        for i, node_id in enumerate([node_analysis[n_key].node_id for n_key in node_analysis if not n_key in parameters.HARDCODED_PLOT_EXCLUDE]):
            plt.annotate(node_id, (x_ax[i], y_ax[i]))
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label)
        
        # Avg PDR vs Avg Missed Bcasts
        x_ax = node_avg_missed_msgs
        y_ax = node_self_PDR
        title = "NODE: Avg PDR vs Avg Missed Bcasts"
        x_label = "Avg Missed Bcasts"
        y_label = "Avg PDR"
        for i, node_id in enumerate([node_analysis[n_key].node_id for n_key in node_analysis if not n_key in parameters.HARDCODED_PLOT_EXCLUDE]):
            plt.annotate(node_id, (x_ax[i], y_ax[i]))
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label)
    
        # Avg Data Transmissions vs Avg Children
        x_ax = node_avg_children
        y_ax = node_avg_data_transmissions
        title = "NODE: Avg Data Transmissions vs Avg Children"
        x_label = "Avg Children"
        y_label = "Avg Data Transmissions"
        for i, node_id in enumerate([node_analysis[n_key].node_id for n_key in node_analysis if not n_key in parameters.HARDCODED_PLOT_EXCLUDE]):
            plt.annotate(node_id, (x_ax[i], y_ax[i]))
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label)
    
        # Avg Data Transmissions vs Avg CRC Fails
        x_ax = node_avg_crc_receive_fails
        y_ax = node_avg_data_transmissions
        title = "NODE: Avg Data Transmissions vs Avg CRC Fails"
        x_label = "Avg CRC Fails"
        y_label = "Avg Data Transmissions"
        for i, node_id in enumerate([node_analysis[n_key].node_id for n_key in node_analysis if not n_key in parameters.HARDCODED_PLOT_EXCLUDE]):
            plt.annotate(node_id, (x_ax[i], y_ax[i]))
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label)
    
        # Lifetime (Cycles) vs Avg HC
        x_ax = node_avg_HC
        y_ax = node_lifetime_cycles
        title = "NODE: Lifetime vs Avg HC"
        x_label = "Avg HC"
        y_label = "Lifetime (Cycles)"
        for i, node_id in enumerate([node_analysis[n_key].node_id for n_key in node_analysis if not n_key in parameters.HARDCODED_PLOT_EXCLUDE]):
            plt.annotate(node_id, (x_ax[i], y_ax[i]))
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label)
        
        # Lifetime (Cycles) vs Avg Num Children
        x_ax = node_avg_children
        y_ax = node_lifetime_cycles
        title = "NODE: Lifetime vs Avg Children"
        x_label = "Avg Children"
        y_label = "Lifetime (Cycles)"
        for i, node_id in enumerate([node_analysis[n_key].node_id for n_key in node_analysis if not n_key in parameters.HARDCODED_PLOT_EXCLUDE]):
            plt.annotate(node_id, (x_ax[i], y_ax[i]))
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label)
        
        # Avg WTB vs Avg Num Children
        x_ax = node_avg_HC
        y_ax = node_avg_WTB
        title = "NODE: Avg WTB vs Avg HC"
        x_label = "Avg HC"
        y_label = "Avg WTB"
        for i, node_id in enumerate([node_analysis[n_key].node_id for n_key in node_analysis if not n_key in parameters.HARDCODED_PLOT_EXCLUDE]):
            plt.annotate(node_id, (x_ax[i], y_ax[i]))
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label)

        # Avg Data Trans vs Avg HC
        x_ax = node_avg_HC
        y_ax = node_avg_data_transmissions
        title = "Avg Data Trans vs Avg HC"
        x_label = "Avg HC"
        y_label = "Avg Data Transmissions"
        for i, node_id in enumerate([node_analysis[n_key].node_id for n_key in node_analysis if not n_key in parameters.HARDCODED_PLOT_EXCLUDE]):
            plt.annotate(node_id, (x_ax[i], y_ax[i]))
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label)

        # *********** MSG PLOTS ***********

        x_ax = msg_parent_rssi
        y_ax = msg_crc_fails
        title = "MSG: CRC Fails vs Parent RSSI"
        x_label = "Parent RSSI"
        y_label = "CRC Fails"
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label, False, 0.1)
        
        x_ax = msg_parent_rssi
        y_ax = msg_data_transmissions
        title = "MSG: Data Transmissions vs Parent RSSI"
        x_label = "Parent RSSI"
        y_label = "Data Transmissions"
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label, False, 0.1)
        
        x_ax = msg_data_transmissions
        y_ax = msg_crc_fails
        title = "MSG: CRC Fails vs Data Transmissions"
        x_label = "Data Transmissions"
        y_label = "CRC Fails"
        plot_w_lin_reg(x_ax, y_ax, title, x_label, y_label, False, 0.1)

        # Plot HC and number of children connections per cycle per node
        if parameters.PLOT_NODE_SPECIFIC_CONNECTIONS:
            for n_key in node_analysis:
                node = node_analysis[n_key]
                plot_cycles = range(sum(bcast_info_overflow.values()) + len(bcast_info_overflow)) # Add len since values are MAX (not length) and 0 start
                plot_hop_count = []
                plot_children = []
                for bi in bcast_info_overflow:
                    for bn in range(bcast_info_overflow[bi] + 1):
                        if bi in node.cycle_hops and bn in node.cycle_hops[bi]:
                            plot_hop_count.append(node.cycle_hops[bi][bn])
                        else:
                            plot_hop_count.append(0)
                        if bi in node.cycle_children and bn in node.cycle_children[bi]:
                            plot_children.append(len(node.cycle_children[bi][bn]))
                        else:
                            plot_children.append(0)
                figure, axis = plt.subplots(2, sharex=True)
                # GNODE
                axis[0].scatter(plot_cycles, plot_hop_count)
                axis[0].set_title("Node " + str(n_key) + ": Hop Count vs Cycle")
                axis[0].set_ylabel("Hop Count")
                # SNODE
                axis[1].scatter(plot_cycles, plot_children)
                axis[1].set_title("Num Children vs Cycle")
                axis[1].set_xlabel("Cycle")
                axis[1].set_ylabel("Num Children")
                plt.show()
    
        # *********** TOPOLOGY PLOTS ***********
    
        # Plot network with parent-child connections
        if len(parameters.HARDCODED_NODE_LOCS) > 0:
            # nodes
            locs_flipped = {node: (y, x) for (node, (x,y)) in parameters.HARDCODED_NODE_LOCS.items()}
            nx.draw_networkx_nodes(G, locs_flipped, node_size=parameters.NETWORK_DRAW_OPTIONS["node_size"], \
                node_color=parameters.NETWORK_DRAW_OPTIONS["node_color"], edgecolors=parameters.NETWORK_DRAW_OPTIONS["node_edge_color"],
                linewidths=parameters.NETWORK_DRAW_OPTIONS["node_width"])
            # edges
            weighted_edges = [G[u][v]['weight'] for u,v in G.edges()]
            nx.draw_networkx_edges(G, locs_flipped, width=weighted_edges, connectionstyle="angle3")
            # labels
            nx.draw_networkx_labels(G, locs_flipped, font_size=parameters.NETWORK_DRAW_OPTIONS["node_font_size"])

            ax = plt.gca()
            plt.axis("off")
            plt.tight_layout()
            plt.title("Combined Parent Map")
            plt.xlabel("GPS X")
            plt.ylabel("GPS Y")
            plt.show()
            
            # Per Node Plots
            if parameters.PLOT_NODE_SPECIFIC_MAPS:
                for n_key in node_analysis:
                    G.clear()
                    # Show all the nodes
                    for n in node_analysis:
                        G.add_edge(n, n, weight=0)
                    for p in node_analysis[n_key].paths:
                        connections = node_analysis[n_key].connections[p[-2]] # Percentage connections of this path, p[1] is the first parent
                        for node in reversed(range(1, len(p))):
                            G.add_edge(p[node], p[node - 1], weight=connections/parameters.PLOT_LOCS_WEIGHT_SCALAR_SPECIFIC)
                    
                    # nodes
                    nx.draw_networkx_nodes(G, locs_flipped, node_size=parameters.NETWORK_DRAW_OPTIONS["node_size"], \
                        node_color=parameters.NETWORK_DRAW_OPTIONS["node_color"], edgecolors=parameters.NETWORK_DRAW_OPTIONS["node_edge_color"],
                        linewidths=parameters.NETWORK_DRAW_OPTIONS["node_width"])
                    # edges
                    weighted_edges = [G[u][v]['weight'] for u,v in G.edges()]
                    nx.draw_networkx_edges(G, locs_flipped, width=weighted_edges, connectionstyle="angle3")
                    # labels
                    nx.draw_networkx_labels(G, locs_flipped, font_size=parameters.NETWORK_DRAW_OPTIONS["node_font_size"])

                    ax = plt.gca()
                    plt.axis("off")
                    plt.tight_layout()
                    plt.title("Node " + str(n_key) + " Map")
                    plt.xlabel("GPS X")
                    plt.ylabel("GPS Y")
                    plt.show()       

if __name__ == "__main__":
    main()